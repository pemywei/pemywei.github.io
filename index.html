<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">


<meta name="keywords" content="Xiangpeng Wei, Wei Xiangpeng, pemywei"> 
<meta name="google-site-verification" content="yy_3iiS_X6pJdegdwitJMrH0LRLHXwpjrV9RKLXxKjg">
<meta name="google-site-verification" content="yy_3iiS_X6pJdegdwitJMrH0LRLHXwpjrV9RKLXxKjg">
<link rel="stylesheet" href="./jemdoc.css" type="text/css">
<title>Xiangpeng Wei - Homepage</title>
<script async="" src="./analytics.js"></script><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-87320911-1', 'auto');
  ga('send', 'pageview');

</script>
<script type="text/javascript" src="./jquery-1.12.4.min.js"></script></head>
<body>
<div id="layout-content" style="margin-top:25px">

<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">					
					<h1>Xiangpeng Wei (魏相鹏)</h1><h1>
				</h1></div>

				<!-- <h3>NLP Researcher</h3> -->
				<p>
					<i> Algorithm Expert at Alibaba DAMO Academy </i>
					<br>
					<br>
					Location: Hangzhou, China | Email: pemywei@gmail.com
					<br>
					[<a href="https://scholar.google.com.hk/citations?user=KnLk78UAAAAJ&hl=zh-CN" target="_blank">Google Scholar</a>] [<a href="https://dblp.dagstuhl.de/pers/hd/w/Wei:Xiangpeng" target="_blank">DBLP</a>] <br>
					<br>
					Feel free and welcome to contact for intern positions and possible collaboration!
				</p>
			</td>
			<td>
				<img src="./me.png" border="0" width="160"><br>
			</td>
		</tr><tr>
	</tr></tbody>
</table>

<h2>Biography</h2>
<p>
	About Me: I am currently an Algorithm Expert in the Language Technology Lab at Alibaba DAMO Academy. I received my Ph.D. degree from the University of Chinese Academy of Sciences (UCAS) in 2021, supervised by Prof. Yue Hu.

</p>
<p>My research interests include natural language generation and multilingual applications. Once awarded the Outstanding Paper Award at ACL 2022 and recognized as a Beijing Excellent Graduate. Currently, I focus on pretraining a polyglot large language model.
</p>

<h2>Education</h2>
<ul>
	<li>
		2016.9-2021.6 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ph.D in Computer Science, <a href="https://www.ucas.ac.cn/" target="_blank">University of Chinese Academy of Sciences.</a>
	</li>
	<li>
		2012.9-2016.6 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; B.S. in Software Engineering, <a href="https://www.dlut.edu.cn/" target="_blank">Dalian University of Technology.</a>
	</li>
</ul>

<h2>News</h2>
<ul>
	<li>
		2023.07 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; We are pleased to announce the launch of our new polyglot large language model, PolyLM.
	</li>
	<li>
		2022.05 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Our paper "Learning to Generalize to More: Continuous Semantic Augmentation for Neural Machine Translation" has been selected as an outstanding paper in ACL 2022.
	</li>
	<li>
		2022.02 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Our paper "Learning to Generalize to More: Continuous Semantic Augmentation for Neural Machine Translation" has been accepted to ACL 2022 main conference.
	</li>
</ul>

<h2>Publications </h2>
<ul>
	<li>
		<b>Xiangpeng Wei</b>, Haoran Wei, Huan Lin, Tianhao Li, Pei Zhang, Xingzhang Ren, Mei Li, Yu Wan, Zhiwei Cao, Binbin Xie, Tianxiang Hu, Shangjie Li, Binyuan Hui, Bowen Yu, Dayiheng Liu, Baosong Yang, Fei Huang, Jun Xie.
		"PolyLM: An Open Source Polyglot Large Language Model". [<a href="https://arxiv.org/pdf/2307.06018.pdf" target="_blank">paper</a>][<a href="https://huggingface.co/DAMO-NLP-MT" target="_blank">huggingface</a>][<a href="https://modelscope.cn/models/damo/nlp_polylm_13b_text_generation/summary" target="_blank">modelscope</a>]
	</li>
	<li>
		<b>Xiangpeng Wei</b>, Heng Yu, Yue Hu, Rongxiang Weng, Weihua Luo, Jun Xie and Rong Jin.
		"Learning to Generalize to More: Continuous Semantic Augmentation for Neural Machine Translation". In ACL 2022. Outstanding Paper Award. [<a href="https://arxiv.org/pdf/2204.06812.pdf" target="_blank">paper</a>][<a href="https://github.com/pemywei/csanmt" target="_blank">code</a>][<a href="https://github.com/pemywei/csanmt/blob/main/csanmt_acl2022_slides.pdf" target="_blank">slides</a>]
	</li>
	<li>
		<b>Xiangpeng Wei</b>, Yue Hu, Rongxiang Weng, Luxi Xing, Heng Yu and Weihua Luo.
		"On Learning Universal Representations Across Languages". In ICLR 2021. [<a href="https://openreview.net/pdf?id=Uu1Nw-eeTxJ" target="_blank">paper</a>] 
	</li>
	<li>
		<b>Xiangpeng Wei</b>, Heng Yu, Yue Hu, Rongxiang Weng, Luxi Xing and Weihua Luo.
		"Uncertainty-Aware Semantic Augmentation for Neural Machine Translation". In EMNLP 2020. [<a href="https://aclanthology.org/2020.emnlp-main.216/" target="_blank">paper</a>][<a href="https://aclanthology.org/attachments/2020.emnlp-main.216.OptionalSupplementaryMaterial.zip" target="_blank">code</a>]
	</li>
	<li>
		<b>Xiangpeng Wei</b>, Heng Yu, Yue Hu, Yue Zhang, Rongxiang Weng and Weihua Luo.
		"Multiscale Collaborative Deep Models for Neural Machine Translation". In ACL 2020. [<a href="https://arxiv.org/pdf/2004.14021.pdf" target="_blank">paper</a>][<a href="https://github.com/pemywei/MSC-NMT" target="_blank">code</a>]
	</li>
	<li>
			<b>Xiangpeng Wei</b>, Yue Hu, Luxi Xing, Yipeng Wang and Li Gao. "Translating with Bilingual Topic Knowledge for Neural Machine Translation". In AAAI 2019. [<a href="https://wvvw.aaai.org/ojs/index.php/AAAI/article/view/4711" target="_blank">paper</a>]
	</li>
	<li>
			<b>Xiangpeng Wei</b>, Yue Hu, Luxi Xing and Li Gao. "Unsupervised Neural Machine Translation with Future Rewarding". In CoNLL 2019. [<a href="https://www.aclweb.org/anthology/K19-1027.pdf" target="_blank">paper</a>]
	</li>
	<li>
		Rongxiang Weng, Heng Yu, <b>Xiangpeng Wei</b> and Weihua Luo.
		"Towards Enhancing Faithfulness for Neural Machine Translation". In EMNLP 2020.
	</li>
</ul>

<h2>Experience </h2>
<ul>
<li>
	2022.9-Now &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Algorithm Expert, Alibaba DAMO Academy.
</li>
<li>
	2021.7-2022.8 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Senior Algorithm Eigineer, Alibaba DAMO Academy.
</li>
<li>
	2020.5-2021.7 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Intern Algorithm Engineer, Alibaba DAMO Academy.
</li>
<li>
	2019.2-2019.9 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Research Intern, Alibaba DAMO Academy.
</li>
</ul>

<h2>Competitions </h2>
<ul>
<li>
	<a href="http://www.statmt.org/wmt20/" target="_blank"><b>WMT20</b></a>, ranking <b>2nd</b> on both German-to-French and French-to-German news shared tasks. 
</li>
<li>
	<a href="http://ccmt2019.jxnu.edu.cn/page/main1923/pctz.htm" target="_blank"><b>CCMT19</b></a>, ranking <b>2nd</b> on the multi-lingual zero-shot MT task among all constrained systems. 
</li>
</ul>
<h2>Academic Activities </h2>
<ul>
<li>
	Conference program committee: ICLR, EMNLP, ICML, ACL, NeurIPS, NAACL
</li>
<li>
	Talks:
	<p> Learning to Generalize to More: Continuous Semantic Augmentation for Neural Machine Translation. AI TIME. 2022.06.
	<p> On Learning Universal Representations Across Languages. XTREME Team, Google Inc. 2021.04.
	<p> Multiscale Collaborative Deep Models for Neural Machine Translation. DAMO Academy, Alibaba Group. 2020.07.
</li>
</ul>
<h2>Awards </h2>
<ul>
<li>
	2021, Beijing Outstanding Graduates
</li>
<li>
	2021, Zhu Li Yuehua Outstanding Doctoral Scholarship
</li>
<li>
	2020, CAS Presidential Scholarship
</li>
<li>
	2019, National Scholarship
</li>
</ul>
	
<!-- Start of StatCounter Code for Default Guide -->
<script type="text/javascript">
var sc_project=11487137; 
var sc_invisible=1; 
var sc_security="94648ebe"; 
var scJsHost = (("https:" == document.location.protocol) ?
"https://secure." : "http://www.");
document.write("<sc"+"ript type='text/javascript' src='" +
scJsHost+
"statcounter.com/counter/counter.js'></"+"script>");
</script>
<noscript><div class="statcounter"><a title="web counter"
href="http://statcounter.com/" target="_blank"><img
class="statcounter"
src="//c.statcounter.com/11487137/0/94648ebe/1/" alt="web
counter"></a></div></noscript>
<!-- End of StatCounter Code for Default Guide -->
	
<div class="jvectormap-tip"></div></body></html>
